{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharya5/SpeechUnderstanding/blob/main/1_MMS_LID_Inference_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running MMS-LID inference in Colab"
      ],
      "metadata": {
        "id": "Rhm7khm6GskV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Clone fairseq-py and install latest version"
      ],
      "metadata": {
        "id": "2GfxksHDGyJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj2x80SegRzr",
        "outputId": "aca47d5a-1fb6-4d21-e0af-5ed5f5f1645e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35073, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 35073 (delta 0), reused 3 (delta 0), pack-reused 35061\u001b[K\n",
            "Receiving objects: 100% (35073/35073), 25.11 MiB | 10.10 MiB/s, done.\n",
            "Resolving deltas: 100% (25464/25464), done.\n",
            "/content\n",
            "/content/fairseq\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.8)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.23.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.1)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9393 sha256=a822df4791792b50b9f765605c706b47403825701607722bd59d2f1cc019ff85\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t0bpfo28/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=a62d1b99819da4e35b2fd166c183d731c7eff5cb8acf3dcefa26494f6876ca75\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "\n",
        "# Change current working directory\n",
        "!pwd\n",
        "%cd \"/content/fairseq\"\n",
        "!pip install --editable ./\n",
        "!pip install tensorboardX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download MMS-LID model\n",
        "\n"
      ],
      "metadata": {
        "id": "cyk4JvZOHSw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = [\"l126\", \"l256\", \"l512\", \"l1024\", \"l2048\", \"l4017\"]\n",
        "\n",
        "# We will use L126 model which can recognize 126 languages\n",
        "model_name = available_models[0] # l126\n",
        "print(f\"Using model - {model_name}\")\n",
        "print(f\"Visit https://dl.fbaipublicfiles.com/mms/lid/mms1b_{model_name}_langs.html to check all the languages supported by this model.\")\n",
        "\n",
        "! mkdir -p /content/models_lid\n",
        "!wget -P /content/models_lid/{model_name} 'https://dl.fbaipublicfiles.com/mms/lid/mms1b_{model_name}.pt'\n",
        "!wget -P /content/models_lid/{model_name} 'https://dl.fbaipublicfiles.com/mms/lid/dict/l126/dict.lang.txt'\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZ9WG85gZId",
        "outputId": "db59863d-283b-4037-e3fc-9916eb9df957"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using model - l126\n",
            "Visit https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126_langs.html to check all the languages supported by this model.\n",
            "--2024-01-27 08:31:49--  https://dl.fbaipublicfiles.com/mms/lid/mms1b_l126.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.59, 13.227.219.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3856229421 (3.6G) [binary/octet-stream]\n",
            "Saving to: ‘/content/models_lid/l126/mms1b_l126.pt’\n",
            "\n",
            "mms1b_l126.pt       100%[===================>]   3.59G   235MB/s    in 19s     \n",
            "\n",
            "2024-01-27 08:32:09 (190 MB/s) - ‘/content/models_lid/l126/mms1b_l126.pt’ saved [3856229421/3856229421]\n",
            "\n",
            "--2024-01-27 08:32:09--  https://dl.fbaipublicfiles.com/mms/lid/dict/l126/dict.lang.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.10, 13.227.219.59, 13.227.219.33, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 882 [text/plain]\n",
            "Saving to: ‘/content/models_lid/l126/dict.lang.txt’\n",
            "\n",
            "dict.lang.txt       100%[===================>]     882  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-27 08:32:10 (360 MB/s) - ‘/content/models_lid/l126/dict.lang.txt’ saved [882/882]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Prepare manifest files\n",
        "Create a folder on path '/content/audio_samples/' and upload your .wav audio files that you need to recognize e.g. '/content/audio_samples/abc.wav' , '/content/audio_samples/def.wav' etc...\n",
        "\n",
        "Note: You need to make sure that the audio data you are using has a sample rate of 16kHz You can easily do this with FFMPEG like the example below that converts .mp3 file to .flac and fixing the audio sample rate\n",
        "\n",
        "Here, we use three examples - one audio file from English, Hindi, Chinese each."
      ],
      "metadata": {
        "id": "3p5-TQvKHXjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/audio_samples/\n",
        "#for key in [\"en_us\", \"hi_in\", \"cmn_hans_cn\"]:\n",
        "#  !wget -O /content/audio_samples/tmp.mp3 /content/audio_samples/1.mp3\n",
        "!ffmpeg -hide_banner -loglevel error -y -i   /content/audio_samples/1.mp3 -ar 16000 /content/audio_samples/1.wav\n",
        "!ffmpeg -hide_banner -loglevel error -y -i   /content/audio_samples/2.mp3 -ar 16000 /content/audio_samples/2.wav\n",
        "\n",
        "! mkdir -p /content/audio_samples/\n",
        "\n"
      ],
      "metadata": {
        "id": "cnim4bokprbB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/manifest/\n",
        "import os\n",
        "with open(\"/content/manifest/dev.tsv\", \"w\") as ftsv, open(\"/content/manifest/dev.lang\", \"w\") as flang:\n",
        "  ftsv.write(\"/\\n\")\n",
        "\n",
        "  for fl in os.listdir(\"/content/audio_samples/\"):\n",
        "    if not fl.endswith(\".wav\"):\n",
        "      continue\n",
        "    audio_path = f\"/content/audio_samples/{fl}\"\n",
        "    # duration should be number of samples in audio. For inference, using a random value should be fine.\n",
        "    duration = 1234\n",
        "    ftsv.write(f\"{audio_path}\\t{duration}\\n\")\n",
        "    flang.write(\"eng\\n\") # This is the \"true\" language for the audio. For inference, using a random value should be fine.\n"
      ],
      "metadata": {
        "id": "C2QcjRT-BArW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Run Inference and transcribe your audio(s)\n"
      ],
      "metadata": {
        "id": "44UvHjmMI28Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/fairseq\"\n",
        "os.environ[\"PREFIX\"] = \"INFER\"\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "os.environ[\"USER\"] = \"mms_lid_user\"\n",
        "\n",
        "!python3 examples/mms/lid/infer.py /content/models_lid/{model_name} --path /content/models_lid/{model_name}/mms1b_l126.pt \\\n",
        "  --task audio_classification  --infer-manifest /content/manifest/dev.tsv --output-path /content/manifest/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8N1RKtBiw5V",
        "outputId": "54600166-c54d-410a-a2c4-e068377d50ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-27 08:43:16.479542: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-27 08:43:16.479665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-27 08:43:16.615000: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-27 08:43:16.867192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-27 08:43:19.199383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "| loading model from /content/models_lid/l126/mms1b_l126.pt\n",
            "2024-01-27 08:43:28 | INFO | fairseq.tasks.audio_classification | Using dict_path : /content/models_lid/l126/dict.lang.txt\n",
            "2024-01-27 08:43:28 | INFO | root | === Number of labels = 126\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "2024-01-27 08:43:52 | INFO | fairseq.data.audio.raw_audio_dataset | loaded 2, skipped 0 samples\n",
            "2024-01-27 08:43:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-01-27 08:43:52 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2024-01-27 08:43:52 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = True\n",
            "2024-01-27 08:43:52 | INFO | fairseq.tasks.fairseq_task | batches will be rebuilt for each epoch\n",
            "2024-01-27 08:43:52 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "2it [00:02,  1.14s/it]\n",
            "Outputs will be located at - /content/manifest//predictions.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"----- INPUT FILES -----\")\n",
        "! tail -n +2 /content/manifest/dev.tsv\n",
        "\n",
        "print(\"\\n----- TOP-K PREDICTONS WITH SCORE -----\")\n",
        "! cat /content/manifest//predictions.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f7FROqiC-2z",
        "outputId": "e3f08547-00c1-44ce-f28e-4c6e4aeadf30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- INPUT FILES -----\n",
            "/content/audio_samples/1.wav\t1234\n",
            "/content/audio_samples/2.wav\t1234\n",
            "\n",
            "----- TOP-K PREDICTONS WITH SCORE -----\n",
            "[[\"hin\", 0.9778025150299072], [\"urd\", 0.007791353855282068], [\"pan\", 0.007627932354807854]]\n",
            "[[\"guj\", 0.4081483483314514], [\"mar\", 0.36899444460868835], [\"kan\", 0.05524643138051033]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzHHmno5DZC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMbKiNWbjWg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}